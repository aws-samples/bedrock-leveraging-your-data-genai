{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c05e65-82da-4445-9577-f56809005b0b",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) using Bedrock in SageMaker\n",
    "\n",
    "In this notebook we demonstrate how to use Retrieval Augmented Generation (RAG) to build a question-and-answer chatbot to converse with the **SEC Schedule 14A document** using Bedrock Models in SageMaker.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is used to retrieve data from outside a bedrock model and augment your prompts by adding the relevant retrieved data in context. For more information about RAG model architectures, see [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401).\n",
    "\n",
    "With RAG, the external data used to augment your prompts can come from multiple data sources, such as a document repositories, databases, or APIs. The first step is to convert your documents and any user queries into a compatible format to perform relevancy search. To make the formats compatible, a document collection, or knowledge library, and user-submitted queries are converted to numerical representations using embedding language models. Embedding is the process by which text is given numerical representation in a vector space. RAG model architectures compare the embeddings of user queries within the vector of the knowledge library. The original user prompt is then appended with relevant context from similar documents within the knowledge library. This augmented prompt is then sent to the bedrock model. You can update knowledge libraries and their relevant embeddings asynchronously.\n",
    "\n",
    "In the previous sections of this workshop, you enabled Bedrock Models and tested these models for various Natural Language Processing (NLP) tasks such as text summarization, common sense reasoning, translation and question and answering. In this section, we will use this Bedrock endpoints to create vector embeddings that are stored in Amazon OpenSearch. We then use these embeddings in a RAG-model for a question-and-answer chatbot. The diagram below depicts this architecture.\n",
    "\n",
    "We will also use **LangChain**, an opensource framework for developing and interfacing with applications powered by language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3949f7e8-34a7-4f87-b359-853c48d17f0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Rag Architecture](../images/10-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b267d-6e22-48ec-8ce4-d3f9f6cf4fc3",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "The following are the prerequisites for this notebook:\n",
    "1. Enable Access to Bedrock Models, We would use Claude V2 and Titan for embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d8c09-007d-4c00-86f6-6d26932d1d88",
   "metadata": {},
   "source": [
    "## Install Required Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c459e-6031-4bda-a882-905f8491ee68",
   "metadata": {
    "tags": []
   },
   "source": [
    "_*IMPORTANT*_: Ensure you are running Pythin 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0f9b6-4de9-4896-b0e6-c94ec241a81c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Set Up Kernel and Required Dependencies\n",
    "\n",
    "First, check that the correct kernel is chosen.\n",
    "\n",
    "<img src=\"img/CheckDataScience30_python3.png\" width=\"300\"/>\n",
    "\n",
    "You can click on that to see and check the details of the image, kernel, and instance type.\n",
    "\n",
    "<img src=\"img/SetupDataScience30_python3.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7426d06-6974-49a0-98f8-e8135eabf2b2",
   "metadata": {},
   "source": [
    "# NOTE:  YOU CANNOT CONTINUE UNTIL THE KERNEL IS STARTED\n",
    "# ### PLEASE WAIT UNTIL THE KERNEL IS STARTED BEFORE CONTINUING!!! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2cedb2-067a-40a1-99ed-7acccbc2700f",
   "metadata": {},
   "source": [
    "# Use `Shift+Enter` to Run Each Cell\n",
    "\n",
    "Use `Shift+Enter` on the cell below to see the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d0d4b-1a67-42d7-9c53-9c89ae6e0d34",
   "metadata": {},
   "source": [
    "# Click `Kernel` => `Restart Kernel and Run All Cells` to Run All Cells\n",
    "![](img/restart-kernel-and-run-all-cells.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4673292a-807d-43cf-a654-f9c4b6296e97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.9.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Get the Python version.\n",
    "python_version = sys.version_info\n",
    "\n",
    "# Check if the Python version is above 3.9.\n",
    "if python_version.major < 3 or python_version.minor < 9:\n",
    "  # Raise an error message if the Python version is not above 3.9.\n",
    "  raise Exception(\"Python version must be above 3.9.\")\n",
    "\n",
    "# Print a success message if the Python version is above 3.9.\n",
    "print(\"Python version is above 3.9.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3476b4c-559b-4da1-b524-14ca011e04f9",
   "metadata": {},
   "source": [
    "Begin by installing the required python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d992e1-9188-4977-a485-7092d0b9a6c7",
   "metadata": {},
   "source": [
    "## _==> Please ignore all WARNINGs and ERRORs from the `pip install`'s below. <==_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd04309a-1b91-4060-8ba2-9bfefe758294",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb06ec0-d912-4a51-8b2e-8c898dd4041d",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5c5c149-aa23-496e-b036-10ffc73e5615",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Setup SageMaker Session\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e7ff9b8-5c60-4582-bf1c-ab7b8a4a302b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'BWB_ENDPOINT_URL' (str)\n",
      "Stored 'BWB_PROFILE_NAME' (str)\n",
      "Stored 'BWB_REGION_NAME' (str)\n",
      "https://bedrock-runtime.us-east-1.amazonaws.com\n",
      "bedrock-runtime\n",
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "BWB_REGION_NAME = boto3.Session().region_name   \n",
    "BWB_ENDPOINT_URL = 'https://bedrock-runtime.'+BWB_REGION_NAME+'.amazonaws.com'\n",
    "BWB_PROFILE_NAME = 'bedrock-runtime'\n",
    "# let's use 'store' so you can use these variables in other notebooks\n",
    "%store BWB_ENDPOINT_URL\n",
    "%store BWB_PROFILE_NAME\n",
    "%store BWB_REGION_NAME\n",
    "\n",
    "print(BWB_ENDPOINT_URL)\n",
    "print(BWB_PROFILE_NAME)\n",
    "print(BWB_REGION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318669e-b637-4fde-98ce-45579db01d06",
   "metadata": {},
   "source": [
    "## Let's test Claude V2 to answer a simple question:  What is the largest city in New Hampshire?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "859fd566-1825-49a8-992c-08de22007e52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion': ' The largest city in New Hampshire is Manchester.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "session = boto3.Session() \n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "##from utils import bedrock, print_ww\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "boto3_bedrock = session.client('bedrock-runtime', region, endpoint_url='https://bedrock-runtime.'+region+'.amazonaws.com')\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = boto3.Session().region_name \n",
    "\n",
    "\n",
    "#Here we are identifying the model to use, the prompt, and the inference parameters for the specified model.\n",
    "bedrock_model_id = \"anthropic.claude-v2\" \n",
    "prompt = \"\"\"\n",
    "Human: What is the largest city in New Hampshire?\n",
    "Assistant:\n",
    "\"\"\" \n",
    "#the prompt to send to the model\n",
    "body = json.dumps({\"prompt\": prompt,\"max_tokens_to_sample\": 512,\"stop_sequences\":[],\"temperature\":0,\"top_p\":0.9})\n",
    "\n",
    "#We use Bedrock's invoke_model function to make the call.\n",
    "response = boto3_bedrock.invoke_model(body=body, modelId=bedrock_model_id, accept='application/json', contentType='application/json') #send the payload to Bedrock\n",
    "\n",
    "#This extracts & prints the returned text from the model's response JSON.\n",
    "response_body = json.loads(response.get('body').read()) # read the response\n",
    "print(response_body)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff53f45a-5ec2-40c7-8a15-3ce05cc476e8",
   "metadata": {},
   "source": [
    "## Let's incorporate LangChaing and run some tests with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d3e92ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import langchain \n",
    "from langchain.document_loaders import UnstructuredHTMLLoader,BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,CharacterTextSplitter\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.vectorstores import OpenSearchVectorSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55563d1b-2cf0-43fd-ad8a-c62efd8199a0",
   "metadata": {},
   "source": [
    "### In the next cell, we will take the name of the Amazon OpenSearch cluster we are using in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af133b35-95fe-4ae1-bf09-7341188764c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c5rhjwdc66y1i1n3r9r7.us-east-1.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Set variables for Amazon OpenSearch\n",
    "\n",
    "import boto3\n",
    "cfn_client = boto3.client('cloudformation')\n",
    "stack_name = \"genai-rag-workshop-studio\"\n",
    "\n",
    "response = cfn_client.describe_stacks(\n",
    "    StackName = stack_name,\n",
    ")\n",
    "\n",
    "outputs = response['Stacks'][0]['Outputs'] \n",
    "\n",
    "opensearch_host_id= next(output['OutputValue'] for output in outputs\n",
    "        if output['OutputKey'] == 'ColectionURL')\n",
    "\n",
    "#Please confirm the name of the OpenSearch Index\n",
    "_aos_host = opensearch_host_id\n",
    "_aos_host = _aos_host.replace(\"https://\", \"\")\n",
    "_aos_index = 'fsi-demo-knn'\n",
    "\n",
    "print(_aos_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166d759",
   "metadata": {},
   "source": [
    "## Chunk your Data and Load into Amazon OpenSearch\n",
    "\n",
    "In this section we will chunk the data into smaller documents. Chunking is a technique for splitting large texts into smaller chunks. It is an important step as it optimizes the relevance of the search query for our RAG-model. Which in turn improves the quality of the chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4a2d6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = BSHTMLLoader(\"../data/14A/0000003153-20-000004.html\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4fd7c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document(s) in your data\n",
      "There are 153880 characters in your document\n"
     ]
    }
   ],
   "source": [
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9b604",
   "metadata": {},
   "source": [
    "\n",
    "### Then we select  chunk size and overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb3c6f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 110 documents\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1600, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print (f'Now you have {len(docs)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0b5954f-fc77-45c7-9cb9-b7fc2fe24390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to process document\n",
    "\n",
    "import regex as re\n",
    "\n",
    "def postproc(s):\n",
    "    s = s.replace(u'\\xa0', u' ') # no-break space \n",
    "    s = s.replace('\\n', ' ') # new-line\n",
    "    s = re.sub(r'\\s+', ' ', s) # multiple spaces\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "725f1429-c9c3-49b1-a779-eda1c901232e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc.page_content = postproc(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a10df400-95c0-41ba-a5cf-097af738d30a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='UNITED STATESSECURITIES AND EXCHANGE COMMISSIONWASHINGTON, D.C. 20549SCHEDULE 14A INFORMATIONProxy Statement Pursuant To Section 14(a)of the Securities Exchange Act of 1934xFiled by the RegistrantoFiled by a party other than the RegistrantCheck the appropriate box:oPreliminary proxy statementoConfidential, for use of the Commission only (as permitted by Rule 14a-6(e)(2))xDefinitive proxy statementoDefinitive additional materialsoSoliciting material under Rule 14a-12ALABAMA POWER COMPANY(Name of Registrant as Specified in Its Charter)(Name of Person(s) Filing Proxy Statement, if Other Than the Registrant)Payment of Filing Fee (Check the appropriate box):xNo fee required. oFee computed on table below per Exchange Act Rules 14a-6(i)(1) and 0-11. (1)Title of each class of securities to which transaction applies: (2)Aggregate number of securities to which transaction applies: (3)Per unit price or other underlying value of transaction computed pursuant to Exchange Act Rule 0-11 (set forth the amount on which the filing fee is calculated and state how it was determined): (4)Proposed maximum aggregate value of transaction: (5)Total fee paid: oFee paid previously with preliminary materials.oCheck box if any part of the fee is offset as provided by Exchange Act Rule 0-11(a)(2) and identify the filing for which the offsetting fee was paid previously. Identify the previous filing by registration statement number, or the Form or Schedule and the date of its filing. (1)Amount Previously Paid: (2)Form, Schedule or Registration Statement No.: (3)Filing', metadata={'source': '../data/14A/0000003153-20-000004.html', 'title': 'None'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the first document for correctness\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a0c0820-0080-4676-94a3-38594470ebd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limit the number of total chunks to 1000\n",
    "MAX_DOCS = 1000\n",
    "if len(docs) > MAX_DOCS:\n",
    "    docs = docs[:MAX_DOCS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9342b6-db4d-4509-b0c4-10c8ca08b809",
   "metadata": {},
   "source": [
    "### Prior to populating a vector store, compute embedding to validate the smoothness / no exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04666cc-99c4-4204-a316-9a24dec99333",
   "metadata": {},
   "source": [
    "We create the embeddings object and batch the create the document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59334838-6a06-4062-abe1-648475a7b6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<botocore.client.BedrockRuntime object at 0x7f521c8e8100> region_name=None credentials_profile_name=None model_id='amazon.titan-embed-text-v1' model_kwargs=None endpoint_url=None\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.load.dump import dumps\n",
    "import boto3\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "session = boto3.Session() \n",
    "boto3_bedrock = session.client('bedrock-runtime', region, endpoint_url='https://bedrock-runtime.'+region+'.amazonaws.com')\n",
    "embeddings = BedrockEmbeddings(client=boto3_bedrock)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b2843",
   "metadata": {},
   "source": [
    "### Create embeddings of your documents to get ready for semantic search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c8e830f-bf12-41fd-b500-e3a7ee30661d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "\n",
    "service = \"aoss\"\n",
    "credentials = boto3.Session().get_credentials()\n",
    "\n",
    "# Create AWS4Auth object\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    region,\n",
    "    service,\n",
    "    session_token=credentials.token,\n",
    ")\n",
    "\n",
    "\n",
    "docsearch = OpenSearchVectorSearch.from_texts(\n",
    "    texts=[d.page_content for d in docs],\n",
    "    embedding=embeddings,\n",
    "    metadatas=[d.metadata for d in docs],\n",
    "    opensearch_url=[{'host': _aos_host, 'port': 443}],\n",
    "    index_name=_aos_index,\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    pre_delete_index=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=100000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35dcd9",
   "metadata": {},
   "source": [
    "## Question answering over Documents \n",
    "\n",
    "So far, we have chunked a large document into smaller ones, created vector embedding and stored them in an OpenSearch Vector Database. Now, we can answer questions over this document data.\n",
    "\n",
    "Since we have created an index over the data, we can do a semantic search over the documents; this way only the most relevant documents to answer the question are passed via the prompt to the Large Language Model (LLM). You save both time and money by not passing all the documents to the LLM.\n",
    "\n",
    "We use langchains **question_answering** `stuff` document chain in this example. Further details on Document Chains can be found by visiting the langchain [documentation, here](https://python.langchain.com/docs/modules/chains/document/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c15dbb66-0c50-4839-b616-54b1b20e124f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import json\n",
    "\n",
    "\n",
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={\"max_tokens_to_sample\": 200}\n",
    ")\n",
    "\n",
    "\n",
    "class SageMakerLLMContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        # input_str = json.dumps({prompt: prompt, **model_kwargs})\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        # return response_json[0][\"generated_text\"]\n",
    "        return response_json['generated_texts'][0]\n",
    "\n",
    "\n",
    "sagemaker_llm_content_handler= SageMakerLLMContentHandler()\n",
    "\n",
    "chain = load_qa_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f560cce3-5891-41ef-b6cb-59efbafe2056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Based on the provided context, the directors mentioned are:\\n\\n- Anthony A. Joseph\\n- Robert D. Powers'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who are the directors?\"\n",
    "sim_docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "chain.run(input_documents=sim_docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f67ea7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Based on the context provided, the nominees for election as directors are:\\n\\n- James F. Shackleford III\\n- Phillip M. Webb  \\n- And 7 other unnamed directors that are part of the current board.\\n\\nThe information provides background and descriptions for James F. Shackleford III and Phillip M. Webb, but does not name the other 7 nominees.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who are the nominees?\"\n",
    "sim_docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "chain.run(input_documents=sim_docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "485bc94c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How old is Mark A. Crosswhite?\n",
      "A:  Based on the context provided, Mark A. Crosswhite is 57 years old. The text states \"Mr. Crosswhite, 57, is Chairman, President, and Chief Executive Officer of the Company.\" This indicates that Mark A. Crosswhite is 57 years old.\n",
      "\n",
      "---\n",
      "\n",
      "Q: What is Mark A. Crosswhite current position and what is the name of the organization he/she currently works for?\n",
      "A:  Based on the context provided, Mark A. Crosswhite currently serves as the Chairman, President, and Chief Executive Officer of the Company. The name of the organization he works for is not explicitly stated in the passages.\n",
      "\n",
      "---\n",
      "\n",
      "Q: How old is Phillip M. Webb?\n",
      "A:  Based on the context provided, Phillip M. Webb is 62 years old. The passage states \"Mr. Webb, 62, is President of Webb Concrete and Building Materials, a position he has held since 1982.\" This indicates that Phillip M. Webb is currently 62 years old.\n",
      "\n",
      "---\n",
      "\n",
      "Q: What is Phillip M. Webb current position and what is the name of the organization he/she currently works for?\n",
      "A:  Based on the context provided, Phillip M. Webb's current position is President of Webb Concrete and Building Materials, a position he has held since 1982.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for person in ['Mark A. Crosswhite', 'Phillip M. Webb']:\n",
    "    for query_template in [\n",
    "                    \"How old is {PERSON}?\",\n",
    "                    \"What is {PERSON} current position and what is the name of the organization he/she currently works for?\"\n",
    "                 ]:\n",
    "    \n",
    "        query = query_template.format(PERSON=person)\n",
    "        print('Q:', query)\n",
    "\n",
    "        sim_docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "        answer = chain.run(input_documents=sim_docs, question=query)    \n",
    "        print('A:', answer)\n",
    "        print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165f5ff-d6d5-414e-a4de-b465798a42a8",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bdd1c-d514-4a71-8595-ed9312431261",
   "metadata": {
    "tags": []
   },
   "source": [
    "## If you are running these notebooks as part of a AWS workshop, The host of your event will take care of the clean up. \n",
    "\n",
    "## If you are running the notebooks within your own account, you would need to delete the CloudFormation Template to remove all the components deployed. Feel free to download a copy of these notebooks for your reference"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
