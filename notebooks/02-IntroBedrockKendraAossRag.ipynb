{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db4fedea-f805-4083-90c3-3b36bd17d103",
   "metadata": {},
   "source": [
    "# Amazon Bedrock, Kendra, OpenSearch using Langchain\n",
    "\n",
    "In these notebooks, you will use the [`boto3` Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to work with [Amazon Bedrock](https://aws.amazon.com/bedrock/) Foundational Models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17af7da-9458-4926-8002-7e6bbb58205f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "### ⚠️ Please make sure to use this notebook in an account and Region with Bedrock enable\n",
    "### ⚠️ These notebook require the **`Data Science 3.0`** kernel in Amazon SageMaker Studio.\n",
    "### ⚠️ If you plan to use a third-party foundation model from Anthropic, AI21, etc, you will need to allow-list the account for the 3rd party model provider.  This may require a few days, so please plan ahead.\n",
    "### ⚠️ Some of the samples may be hard-coded to a given region. ***Please do not can change the region.***\n",
    "### ⚠️ Be sure the role used by SageMaker has the policies for ***Full Access*** to Bedrock and Kendra. \n",
    "### ⚠️ We will use a knowledge base created in Bedrock Console to compare the results with Kendra Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305eb3a-bcbd-49e5-b325-81e85e7ce9cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the boto3 client\n",
    "\n",
    "Interaction with the Bedrock API is done via the AWS SDK for Python: [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html).\n",
    "\n",
    "#### Use the default credential chain\n",
    "\n",
    "If you are running this notebook from [Amazon Sagemaker Studio](https://aws.amazon.com/sagemaker/studio/) and your Sagemaker Studio [execution role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) has permissions to access Bedrock you can just run the cells below as-is. This is also the case if you are running these notebooks from a computer whose default AWS credentials have access to Bedrock.\n",
    "\n",
    "#### Use a different AWS Region\n",
    "\n",
    "If you're running this notebook from your own computer or a SageMaker notebook in a different AWS Region from where Bedrock is set up, you can un-comment the `os.environ['AWS_DEFAULT_REGION']` line below and specify the region to use.\n",
    "\n",
    "#### Use a specific profile\n",
    "\n",
    "In case you're running this notebook from your own computer where you have setup the AWS CLI with multiple profiles, and the profile which has access to Bedrock is not the default one, you can un-comment the `os.environ['AWS_PROFILE']` line below and specify the profile to use.\n",
    "\n",
    "#### Use a different role\n",
    "\n",
    "In case you or your company has setup a specific, separate [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) to access Bedrock, you can specify it by un-commenting the `os.environ['BEDROCK_ASSUME_ROLE']` line below. Ensure that your current user or role have permissions to [assume](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html) such role.\n",
    "\n",
    "#### Use a custom service endpoint URL\n",
    "\n",
    "If you need to use a custom [service endpoint URL](https://docs.aws.amazon.com/general/latest/gr/rande.html) to access Bedrock as part of the preview, you'll need to un-comment and edit the `os.environ['BEDROCK_ENDPOINT_URL']` line below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea537d-3970-4ffc-afeb-598965e442b3",
   "metadata": {},
   "source": [
    "### Please follow the next lines to download, unzip and install the libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4d72672-cf69-43b4-9aee-3c9d0c004997",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.32.40 requires botocore==1.34.40, but you have botocore 1.34.37 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5553a-713d-430c-b0dd-922a35c6db6d",
   "metadata": {},
   "source": [
    "## Let's start by listing the different models supported by bedrock on this account. If you want to use a model that is not listed here, please see the Bedrock documentation on how to get access to more models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65f7b4d0-b387-4c5f-b249-f27f88a3af0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon.titan-tg1-large\n",
      "amazon.titan-image-generator-v1:0\n",
      "amazon.titan-image-generator-v1\n",
      "amazon.titan-embed-g1-text-02\n",
      "amazon.titan-text-lite-v1:0:4k\n",
      "amazon.titan-text-lite-v1\n",
      "amazon.titan-text-express-v1:0:8k\n",
      "amazon.titan-text-express-v1\n",
      "amazon.titan-embed-text-v1:2:8k\n",
      "amazon.titan-embed-text-v1\n",
      "amazon.titan-embed-image-v1:0\n",
      "amazon.titan-embed-image-v1\n",
      "stability.stable-diffusion-xl\n",
      "stability.stable-diffusion-xl-v0\n",
      "stability.stable-diffusion-xl-v1:0\n",
      "stability.stable-diffusion-xl-v1\n",
      "ai21.j2-grande-instruct\n",
      "ai21.j2-jumbo-instruct\n",
      "ai21.j2-mid\n",
      "ai21.j2-mid-v1\n",
      "ai21.j2-ultra\n",
      "ai21.j2-ultra-v1\n",
      "anthropic.claude-instant-v1:2:100k\n",
      "anthropic.claude-instant-v1\n",
      "anthropic.claude-v1\n",
      "anthropic.claude-v2:0:18k\n",
      "anthropic.claude-v2:0:100k\n",
      "anthropic.claude-v2:1:18k\n",
      "anthropic.claude-v2:1:200k\n",
      "anthropic.claude-v2:1\n",
      "anthropic.claude-v2\n",
      "cohere.command-text-v14:7:4k\n",
      "cohere.command-text-v14\n",
      "cohere.command-light-text-v14:7:4k\n",
      "cohere.command-light-text-v14\n",
      "cohere.embed-english-v3\n",
      "cohere.embed-multilingual-v3\n",
      "meta.llama2-13b-chat-v1:0:4k\n",
      "meta.llama2-13b-chat-v1\n",
      "meta.llama2-70b-chat-v1:0:4k\n",
      "meta.llama2-70b-chat-v1\n",
      "meta.llama2-13b-v1:0:4k\n",
      "meta.llama2-13b-v1\n",
      "meta.llama2-70b-v1:0:4k\n",
      "meta.llama2-70b-v1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os \n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "\n",
    "bedrock = boto3.client('bedrock', region, endpoint_url='https://bedrock.'+region+'.amazonaws.com')\n",
    "response = bedrock.list_foundation_models()\n",
    "\n",
    "# Extract modelIds from the response data\n",
    "model_ids = [model[\"modelId\"] for model in response.get('modelSummaries')]\n",
    "\n",
    "# Print the list of modelIds\n",
    "for model_id in model_ids:\n",
    "    print(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a36178-5519-4a5b-aa06-1b467d37c32d",
   "metadata": {},
   "source": [
    "### We need to setup some variables like bedrock end point, region where you have bedrock and the AWS profiles used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a556e63-7d3b-4998-846a-4d54eae80997",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'BWB_ENDPOINT_URL' (str)\n",
      "Stored 'BWB_PROFILE_NAME' (str)\n",
      "Stored 'BWB_REGION_NAME' (str)\n",
      "https://bedrock-runtime.us-east-1.amazonaws.com\n",
      "bedrock-runtime\n",
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "BWB_ENDPOINT_URL = 'https://bedrock-runtime.'+region+'.amazonaws.com'\n",
    "BWB_PROFILE_NAME = 'bedrock-runtime'\n",
    "BWB_REGION_NAME = region    \n",
    "\n",
    "# let's use 'store' so you can use these variables in other notebooks\n",
    "%store BWB_ENDPOINT_URL\n",
    "%store BWB_PROFILE_NAME\n",
    "%store BWB_REGION_NAME\n",
    "\n",
    "#%store -r BWB_ENDPOINT_URL\n",
    "#%store -r BWB_PROFILE_NAME\n",
    "#%store -r BWB_REGION_NAME\n",
    "\n",
    "print(BWB_ENDPOINT_URL)\n",
    "print(BWB_PROFILE_NAME)\n",
    "print(BWB_REGION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c27c53-4027-44ef-899e-5f9531b59cd2",
   "metadata": {},
   "source": [
    "### Next, let's create a bedrock client using boto3. We will run a basic question using titan model. If the response is Manchester, this means we were able to setup bedrock client and session properly!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9430e5c-2bbd-4ccc-8a49-6e373267177b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion': ' The largest city in New Hampshire is Manchester.', 'stop_reason': 'stop_sequence', 'stop': '\\n\\nHuman:'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "#This creates a Bedrock client.\n",
    "session = boto3.Session() #sets the profile name to use for AWS credentials\n",
    "\n",
    "bedrock = session.client(\n",
    "    service_name='bedrock-runtime', #creates a Bedrock client for invoking models \n",
    "    region_name=BWB_REGION_NAME,\n",
    "    endpoint_url=BWB_ENDPOINT_URL\n",
    ") \n",
    "\n",
    "#Here we are identifying the model to use, the prompt, and the inference parameters for the specified model.\n",
    "bedrock_model_id = \"anthropic.claude-v2\" #set the model to Claude\n",
    "prompt = \"\"\"\n",
    "Human: What is the largest city in New Hampshire?\n",
    "Assistant:\n",
    "\"\"\" \n",
    "#the prompt to send to the model\n",
    "#body = json.dumps({\"inputText\": prompt, \"textGenerationConfig\": {\"maxTokenCount\": 512, \"stopSequences\": [], \"temperature\": 0, \"topP\": 0.9 } } ) #build the request payload\n",
    "body = json.dumps({\"prompt\": prompt,\"max_tokens_to_sample\": 512,\"stop_sequences\":[],\"temperature\":0,\"top_p\":0.9})\n",
    "\n",
    "#We use Bedrock's invoke_model function to make the call.\n",
    "response = bedrock.invoke_model(body=body, modelId=bedrock_model_id, accept='application/json', contentType='application/json') #send the payload to Bedrock\n",
    "\n",
    "#This extracts & prints the returned text from the model's response JSON.\n",
    "response_body = json.loads(response.get('body').read()) # read the response\n",
    "print(response_body)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c4b5e-936d-486f-91d6-07ac062a23a1",
   "metadata": {},
   "source": [
    "### Intro to LangChain\n",
    "\n",
    "we will show how to make a basic call to Bedrock using LangChain. While similar in functionality to the previous cells where we called Bedrock through Boto3, in this section we will use LangChain so you can compare the two approaches. LangChain can abstract away many of the details of using the Boto3 client, especially when you want to focus on text in and text out. If you need the full control enabled by the Boto3 client, it will always be an option for you. Boto3 may require more code, but it will give you full access to the JSON request and response objects.\n",
    "\n",
    "You can build the application code by copying the code snippets below and pasting into the indicated Python file.\n",
    "\n",
    "#### If \"anthropic.claude-v2\" is not available, please use:  \"amazon.titan-tg1-large\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eeb391e9-a2a8-40db-b515-fad80dd8d17d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Burlington is the largest city in Vermont.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "\n",
    "#This creates a Bedrock client. The client allows us to make standard LangChain calls and have them adapted automatically to work with Bedrock's models.\n",
    "llm = Bedrock( #create a Bedrock llm client\n",
    "    region_name=BWB_REGION_NAME, #sets the region name (if not the default)\n",
    "    endpoint_url=BWB_ENDPOINT_URL, #sets the endpoint URL (if necessary)\n",
    "    model_id=\"anthropic.claude-v2\", model_kwargs={'max_tokens_to_sample':200} #if anthropic is not available use:  \"amazon.titan-tg1-large\"\n",
    ")\n",
    "\n",
    "#With LangChain, the prompt is the minimum payload we need to send to Bedrock. We'll also learn how to set inference parameters in a later lab.\n",
    "prompt = \"What is the largest city in Vermont?\"\n",
    "\n",
    "#the prompt to send to the model\n",
    "body = json.dumps({\"prompt\": prompt,\"max_tokens_to_sample\": 512,\"stop_sequences\":[],\"temperature\":0,\"top_p\":0.9})\n",
    "\n",
    "#Call Bedrock API\n",
    "response_text = llm.invoke(prompt) #return a response to the prompt\n",
    "print(response_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bbb61d-4ac7-473b-8899-1526dd4cfbd6",
   "metadata": {},
   "source": [
    "### Using Kendra and LangChain\n",
    "We let's run get ready for using Kendra and Langchain in our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f0fc8e7-7267-49fb-91ad-07cb18b6d9b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.chains.llm import LLMChain\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ff0a2-753a-4cf2-a69e-a15509eeae3e",
   "metadata": {},
   "source": [
    "### Life is not black and white. These lines will add some colors to Bedrock's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "459f7647-ece0-4de5-af21-7edeae132e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "MAX_HISTORY_LENGTH = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916eb783-5826-4e2d-80bb-80f434b80139",
   "metadata": {},
   "source": [
    "### As final Step we will use Kendra as retriever and the use it within langchain to extend the answer based on the documents store in Kendra\n",
    "## We will follow these steps:\n",
    "1. Define a retriever using the kendra index\n",
    "   - NOTE: ⚠️ Remember to update the kendra Index ID\n",
    "   \n",
    "2. Define a prompt template\n",
    "3. Create a conversation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdeb8a72-0c05-4ab8-9796-51b3a4a39a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_chain():\n",
    "  region = BWB_REGION_NAME \n",
    "  kendra_index_id = '35c6def6-28b7-4214-8b39-ed40e27ebf8b'#this is the \"KENDRA_INDEX_ID\", make sure to get it from Kendra and that the data source is synced up\n",
    "  credentials_profile_name = 'BedrockUserRole'\n",
    "  \n",
    "  retriever = AmazonKendraRetriever(index_id=kendra_index_id,top_k=5,region_name=region)\n",
    "\n",
    "  prompt_template = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "  {context}\n",
    "\n",
    "  Question: {question}\n",
    "  Assistant:\"\"\"\n",
    "  PROMPT = PromptTemplate(\n",
    "      template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "  )\n",
    "  \n",
    "  qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True, #This parameter will show the list of url or links used in the answer provided. Possible values True and False\n",
    "        combine_docs_chain_kwargs={\"prompt\":PROMPT},\n",
    "        verbose=False) #This parameter will show you the sections of the documents used to create the answer. Possible values True and False\n",
    "\n",
    "  return qa\n",
    "def run_chain(chain, prompt: str, history=[]):\n",
    "  return chain({\"question\": prompt, \"chat_history\": history})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2040d9f5-d28d-47d1-9c26-c6c5e45466e7",
   "metadata": {},
   "source": [
    "### You can change the query asked to the LLM, these are some options:\n",
    "    \n",
    "    - How old is Catherine J. Randall?\n",
    "    - How old is Phillip W. Webb?\n",
    "    - What is Angus R. Cooper, III's current position and what is the name of the organization he/she currently works for?\n",
    "    - What year did Robert D. Powers joined the board of directors?\n",
    "    - What committees is Catherine J. Randall a member of?\n",
    "    - What is Phillip W. Webb's current position and what is the name of the organization he/she currently works for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce9a1afc-5373-404c-b957-2c5817786308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> who is older between Powers and Randall? \u001b[92m Based on the document excerpts, Robert D. Powers is 69 years old and Catherine J. Randall is 69 years old. Therefore, Powers and Randall are the same age.\u001b[0m\n",
      "\u001b[92mSources:\n",
      "https://s3.us-east-1.amazonaws.com/genaiconf-2958e600/data/14A/0000003153-20-000004.html\n",
      "https://s3.us-east-1.amazonaws.com/genaiconf-2958e600/data/14A/0000003153-20-000004.html\n",
      "https://s3.us-east-1.amazonaws.com/genaiconf-2958e600/data/14A/0000003153-20-000004.html\n",
      "https://s3.us-east-1.amazonaws.com/genaiconf-2958e600/data/14A/0000003153-20-000004.html\n",
      "https://s3.us-east-1.amazonaws.com/genaiconf-2958e600/data/14A/0000003153-20-000004.html\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "qa = build_chain()\n",
    "query = 'who is older between Powers and Randall?'  #Feel free to change this question\n",
    "chat_history = []\n",
    "if (len(chat_history) == MAX_HISTORY_LENGTH):\n",
    "  chat_history.pop(0)\n",
    "result = run_chain(qa, query, chat_history)\n",
    "chat_history.append((query, result[\"answer\"]))\n",
    "print(\">\", query, end=\" \", flush=True)  \n",
    "print(bcolors.OKGREEN + result['answer'] + bcolors.ENDC)\n",
    "if 'source_documents' in result:\n",
    "  print(bcolors.OKGREEN + 'Sources:')\n",
    "  for d in result['source_documents']:\n",
    "    print(d.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563c361-f075-4c66-b2fd-a9104bd2821b",
   "metadata": {},
   "source": [
    "# Now, let's change the Knowledge based, let's use the one we created in the Bedrock console and compare the results. \n",
    "\n",
    "\n",
    "## ⚠️Warning: Remember to sync up the Knowledge base in Bedrock console and copy the Knowledge Base ID, See the KB Menu in Bedrock's Console\n",
    "### Syncing will take about 2 minutes \n",
    "\n",
    "### Remember: The following Knowledge base is using Amazon OpenSearch Serverless and the embedding selected during the creation of the KB. The result could be different. Those can be improve or fine-tuned by changing several parameter such as: Chunking, Embedding model and others\n",
    "\n",
    "### In the following cell, we will be testing the knowledge base only, you will see no Text2Text Bedrock Model is associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1691d52d-1a21-4cd8-94d8-b7706b6ff08e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please update the knowledge base Id here :) \n",
    "KBaseId = \"AUYJTRQHQ8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed63c6f2-6fe7-4ecf-a86c-148660e84e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O.B. Grayson Hall, Jr. - Director since 2015 Mr. Hall, 62, served as Executive Chairman of Regions Financial Corporation from July 2018 until his retirement in December 2018. He previously served as Chairman of Regions Financial Corporation from May 2013 through July 2018 and Chief Executive Officer from April 2010 through July 2018. He served as President of Regions Financial Corporation from 2009 through December 2017. Mr. Hall serves on the Boards of Directors of Vulcan Materials Company and Great Southern Wood Holdings, Inc. He previously served as a representative on the Federal Advisory Council of the Federal Reserve Bank from 2014 to 2016 and on the Board of the Federal Reserve Bank of Atlanta from 2017 to 2018. Mr. Hall also serves on the Boards of numerous civic organizations, including the Newcomen Society of Alabama and the National Christian Foundation of Alabama and is a trustee of the Crimson Tide Foundation. Mr. Hall's experience in the business community, as well as his civic leadership, make him a valuable member of the Company's Board.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "client = boto3.client('bedrock-agent-runtime')\n",
    "\n",
    "response = client.retrieve(\n",
    "    knowledgeBaseId=KBaseId,\n",
    "    retrievalQuery={\n",
    "        'text': 'who is the oldest person in the board?'\n",
    "    },\n",
    "    retrievalConfiguration={\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 1\n",
    "        }\n",
    "    },\n",
    "    nextToken='string'\n",
    ")\n",
    "\n",
    "# Get retrievalResults \n",
    "results = response['retrievalResults']\n",
    "\n",
    "# Process each result\n",
    "for result in results:\n",
    "  text = result['content']['text']\n",
    "  \n",
    "  # Replace \\t with spaces\n",
    "  text = text.replace('\\t', ' ') \n",
    "  print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d03413-f9f8-4698-aa03-6e9e387584e9",
   "metadata": {},
   "source": [
    "### Now we will use the Text Generation from our Bedrock Model to consolidate the answer we got in the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1d8d71e-dabd-4685-89ff-8f573f4398c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the search results, R. Mitchell Shackleford III, who is 68 years old, appears to be the oldest member of the board.\n",
      "These are the citations from the documents collected in Amazon OpenSearch Serverless:\n",
      "s3://genaiconf-546bcdd0/data/14A_frags/0000003153-20-000004.shackleford.txt\n",
      "s3://genaiconf-546bcdd0/data/14A_frags/0000003153-20-000004.shackleford.pdf\n"
     ]
    }
   ],
   "source": [
    "response = client.retrieve_and_generate(\n",
    "    input={\n",
    "        'text': 'who is the oldest person of the board?'\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': KBaseId,\n",
    "            'modelArn': 'arn:aws:bedrock:'+BWB_REGION_NAME+'::foundation-model/anthropic.claude-v2'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "text = response[\"output\"][\"text\"]\n",
    "\n",
    "print(text)\n",
    "\n",
    "retrieved_references = response['citations'][0]['retrievedReferences']\n",
    "print(\"These are the citations from the documents collected in Amazon OpenSearch Serverless:\")\n",
    "for ref in retrieved_references:\n",
    "    print(ref['location']['s3Location']['uri'])\n",
    "          #['content']['text']+\"/n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112432ae-f449-4de5-b794-a9b3cbcce0b1",
   "metadata": {},
   "source": [
    "### Finally, we will change the question a bit to confirm the information given by our Model is accurate or not, we can also ask the same question when using Kendra Index and check for differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0329070e-bbb3-460d-9a68-b7c9be19062b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Dr. Randall is the oldest member of the board at 69 years old.\n",
      "These are the citations from the documents collected in Amazon OpenSearch Serverless:\n",
      "s3://genaiconf-546bcdd0/data/14A_frags/0000003153-20-000004.randall.pdf\n",
      "s3://genaiconf-546bcdd0/data/14A_frags/0000003153-20-000004.randall.txt\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "response = client.retrieve_and_generate(\n",
    "    input={\n",
    "        'text': 'is Dr. Randall the oldest member of the board?'\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': KBaseId,\n",
    "            'modelArn': 'arn:aws:bedrock:'+BWB_REGION_NAME+'::foundation-model/anthropic.claude-v2'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "text = response[\"output\"][\"text\"]\n",
    "\n",
    "print(text)\n",
    "retrieved_references = response['citations'][0]['retrievedReferences']\n",
    "print(\"These are the citations from the documents collected in Amazon OpenSearch Serverless:\")\n",
    "for ref in retrieved_references:\n",
    "    print(ref['location']['s3Location']['uri'])\n",
    "          #['content']['text']+\"/n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e4be8-d7e5-4414-b112-32bf5de1078f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
